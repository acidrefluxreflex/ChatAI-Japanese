{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install nn_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_name = \"rinna/japanese-gpt-1b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "PRUNE_RATE = 0.2\n",
    "\n",
    "def prune_transform(model: nn.Module) -> nn.Module:\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=PRUNE_RATE)\n",
    "        prune.remove(module, \"weight\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prune_transform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class ChatBot(torch.nn.Module):\n",
    "\n",
    "   def __init__(self):\n",
    "         super(ChatBot, self).__init__()\n",
    "         \n",
    "\n",
    "   #文章生成を行う関数。元になる文章、最大文字数、最小文字数を引数にもつ。\n",
    "   def generate(self, text, max_length, min_length):\n",
    "     token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "     with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            token_ids.to(model.device),\n",
    "            max_new_tokens=max_length,\n",
    "            min_new_tokens=min_length,\n",
    "            do_sample=True,\n",
    "            top_k=500,\n",
    "            top_p=0.95,\n",
    "            padding=\"do_not_pad\",\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            bad_word_ids=[[tokenizer.unk_token_id],\"...\"]\n",
    "        )\n",
    "        output = tokenizer.decode(output_ids.tolist()[0])\n",
    "        return output\n",
    "    \n",
    "\n",
    "   def chat(self):\n",
    "     #プロフィール設定\n",
    "     name = input(\"AIの名前:\")\n",
    "     name_text = f\"あなたは{name}で、名前は{name}といいます。\"\n",
    "     hobby = input(\"AIの趣味:\")\n",
    "     hobby_text = f\"{name}の趣味は{hobby}で、休日は{hobby}をして過ごしています。\"\n",
    "     work = input(\"AIの職業:\")\n",
    "     work_text = f\"{name}の職業は{work}で、{work}として生活しています。\"\n",
    "\n",
    "     setting_text_1 = f\"{name}:「今日はいい天気ですわね。」\"\n",
    "     setting_text_2 = f\"{name}:「わたくしは{name}ですわ！\"\n",
    "\n",
    "     print(\"AIに言いたい事を入力してください。終了したいときは未入力のままEnter\")\n",
    "     userInput = \"ッ\"\n",
    "     text = name_text + hobby_text + work_text + setting_text_1 + setting_text_2 + f\"以下は人間と{name}の会話です。人間:「こんにちは!」{name}:「よろしくお願いしますわ！」人間:「\"\n",
    "     max_length = 25\n",
    "     min_length = 20\n",
    "     \n",
    "\n",
    "     while userInput != \"\":\n",
    "       userInput = input(\">>> \")\n",
    "       start = time.time()\n",
    "       if userInput == \"\":\n",
    "           print(\"会話を終了します\")\n",
    "           break\n",
    "       text += userInput + f\"」{name}:「\"\n",
    "\n",
    "       token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "       print(len(token_ids[0]))\n",
    "       #文字数調節\n",
    "       message_gap = len(token_ids[0]) - max_length\n",
    "       output = self.generate(text,max_length,min_length)\n",
    "       \n",
    "\n",
    "       #半角正則化\n",
    "       text = text.translate(str.maketrans({chr(0xFF01 + i): chr(0x21 + i) for i in range(94)}))\n",
    "   \n",
    "       #今回の応答より前を取得\n",
    "       output = output.replace(text, \"\")\n",
    "       print(output)\n",
    "       #最初の」までを分割する\n",
    "       outputList = []\n",
    "       for l in output:\n",
    "        outputList.append(l)\n",
    "        if l == \"」\":\n",
    "            break\n",
    "       outputSentence = \"\".join(outputList)\n",
    "       text += outputSentence + \"人間:「\"\n",
    "       message = outputSentence.replace(\"」\", \"\")\n",
    "       time.sleep(1)\n",
    "       print(time.time() - start)\n",
    "       print(message)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = ChatBot()\n",
    "bot.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
